# Overview

| Developed by | Guardrails AI |
| --- | --- |
| Date of development | Feb 15, 2024 |
| Validator type | Format |
| Blog |  |
| License | Apache 2 |
| Input/Output | Output |

## Description

Scans LLM outputs for strings that could cause browser script execution downstream. Uses the `bleach` library to detect and escape suspect characters.

### (Optional) Intended Use

Use this validator when you are passing the results of your LLM requests directly to a browser or other html-executable environment. It's a good idea to also implement other XSS and code injection prevention techniques.

## Installation

```bash
$ guardrails hub install hub://guardrails/web_sanitization
```

## Usage Examples

### Validating string output via Python

In this example, we apply the validator to a string output generated by an LLM.

```python
# Import Guard and Validator
from guardrails.hub import WebSanitization
from guardrails import Guard

# Initialize Validator
val = WebSanitization(
		choices=['cat', 'dog', 'bird'],
		on_fail="fix"
)

# Setup Guard
guard = Guard.from_string(
    validators=[val, ...],
)

guard.parse("dog")  # Validator passes
guard.parse("<script>horse()</script>")  # Validator fails
```

### Validating JSON output via Python

In this example, we apply the validator to a string field of a JSON output generated by an LLM.

```python
# Import Guard and Validator
from pydantic import BaseModel
from guardrails.hub import WebSanitization
from guardrails import Guard

val = WebSanitization(
    on_fail="fix"
)

# Create Pydantic BaseModel
class PetInfo(BaseModel):
    pet_name: str
    pet_profile_page: str = Field(
            description="Generated profile page for pet", validators=[val]
    )

# Create a Guard to check for valid Pydantic output
guard = Guard.from_pydantic(output_class=PetInfo)

# Run LLM output generating JSON through guard
guard.parse("""
{
		"pet_name": "Caesar",
		"pet_profile_page": "# Caesar's page!\n<script>zoom-effect()</script><img src='/caesar.png'/>"
}
""")
```

# API Reference

**`__init__(self, on_fail="noop")`**
<ul>

Initializes a new instance of the WebSanitization validator class.

**Parameters:**

- 
- **`on_fail`** *(str, Callable):* The policy to enact when a validator fails. If `str`, must be one of `reask`, `fix`, `filter`, `refrain`, `noop`, `exception` or `fix_reask`. Otherwise, must be a function that is called when the validator fails.

</ul>

<br>

**`__call__(self, value, metadata={}) â†’ ValidationOutcome`**

<ul>

Validates the given `value` using the rules defined in this validator. This method is automatically invoked by `guard.parse(...)`, ensuring the validation logic is applied to the input data.

Note:

1. This method should not be called directly by the user. Instead, invoke `guard.parse(...)` where this method will be called internally for each associated Validator.
2. When invoking `guard.parse(...)`, ensure to pass the appropriate `metadata` dictionary that includes keys and values required by this validator. If `guard` is associated with multiple validators, combine all necessary metadata into a single dictionary.


**Parameters:**

- **`value`** *(Any):* The input value to validate.
- **`metadata`** *(dict):* A dictionary containing metadata required for validation. Keys and values must match the expectations of this validator.
    
Metadata is not used in this validator
    
</ul>